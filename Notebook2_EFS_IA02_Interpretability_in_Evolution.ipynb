{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf1adae",
   "metadata": {},
   "source": [
    "# Evolving Fuzzy Systems (eFS) — Notebook 2  \n",
    "### Interpretability in evolving systems (streaming + concept drift) using `evolvingfuzzysystems`\n",
    "\n",
    "**Seminar (Day 2 / IA02)** — Universidad Carlos III de Madrid- Interpretability for evolving systems\n",
    "\n",
    "**Author:** Prof. Dr. Paulo Vitor de Campos Souza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8dc1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a37532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab / fresh env, uncomment:\n",
    "# !pip -q install evolvingfuzzysystems river scikit-learn matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fc1f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# River: streaming datasets + drift detection\n",
    "from river import datasets\n",
    "from river import drift\n",
    "from river import metrics\n",
    "\n",
    "# Scalers / encoders\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1ad16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolving fuzzy systems library\n",
    "# Key model for Day 2: ENFS_Uni0 (classification)\n",
    "from evolvingfuzzysystems.classification import ENFS_Uni0\n",
    "\n",
    "# Optional: a couple of regression/classification eFS models (if you want to showcase variety)\n",
    "from evolvingfuzzysystems.eFS import ePL, ePL_plus, exTS, Simpl_eTS, eMG, ePL_KRLS_DISCO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97a83e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8923c",
   "metadata": {},
   "source": [
    "## 1) Why “evolving interpretability” matters\n",
    "\n",
    "In offline interpretability, the model is fixed after training.  \n",
    "In streaming / non-stationary settings, **the model structure itself evolves**:\n",
    "\n",
    "- new rules can be created when new patterns appear\n",
    "- redundant rules can be merged / pruned\n",
    "- consequent parameters adapt online\n",
    "\n",
    "In this notebook we make these changes visible with:\n",
    "1) prequential performance (predict → update)  \n",
    "2) drift detection (ADWIN on the error stream)  \n",
    "3) rule evolution: number of rules and rule visualizations (when available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6d3bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cb269",
   "metadata": {},
   "source": [
    "## 2) A stream with controlled concept drift (synthetic, didactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ceea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A classic stream with a controllable drift moment\n",
    "# We use two variants of the same generator as pre- and post-drift concepts.\n",
    "\n",
    "stream = datasets.synth.ConceptDriftStream(\n",
    "    stream=datasets.synth.Agrawal(seed=1),\n",
    "    drift_stream=datasets.synth.Agrawal(seed=2),\n",
    "    position=2500,\n",
    "    width=1000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Quick peek\n",
    "x0, y0 = next(iter(stream))\n",
    "x0, y0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9fe27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9636721",
   "metadata": {},
   "source": [
    "## 3) Utility: stream → numpy buffers (for scaling + plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_to_arrays(stream, n_samples):\n",
    "    \"\"\"Collect first n_samples from a River stream into numpy arrays.\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    it = iter(stream)\n",
    "    for _ in range(n_samples):\n",
    "        x, y = next(it)\n",
    "        # River gives dict features -> convert to ordered vector\n",
    "        X_list.append([x[k] for k in sorted(x.keys())])\n",
    "        y_list.append(y)\n",
    "    X = np.asarray(X_list, dtype=float)\n",
    "    y = np.asarray(y_list)\n",
    "    return X, y, sorted(x.keys())\n",
    "\n",
    "N = 6000\n",
    "X_raw, y_raw, feature_names = stream_to_arrays(stream, N)\n",
    "X_raw.shape, y_raw.shape, feature_names[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a3813",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194acd1b",
   "metadata": {},
   "source": [
    "## 4) Preprocessing (important for Gaussian similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENFS_Uni0 uses distances + Gaussian similarity -> scaling is critical.\n",
    "# We'll use StandardScaler; MinMaxScaler also works well in many eFS.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "\n",
    "# Labels must be integer-encoded: 0..C-1\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "n_features, n_classes, le.classes_[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a10aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22474c",
   "metadata": {},
   "source": [
    "## 5) Model: ENFS_Uni0 (evolving rule base + RLS consequent layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ENFS_Uni0(\n",
    "    n_features=n_features,\n",
    "    n_classes=n_classes,\n",
    "    lambda_ff=0.98,        # forgetting factor (lower -> faster adaptation)\n",
    "    sim_threshold=0.90,    # prune very similar new rules\n",
    "    max_rules=10,          # keep it interpretable in the demo\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Drift detector on the error stream\n",
    "adwin = drift.ADWIN(delta=0.002)\n",
    "\n",
    "# Prequential metric\n",
    "acc = metrics.Accuracy()\n",
    "\n",
    "# Tracking\n",
    "rule_count = []\n",
    "drift_points = []\n",
    "acc_hist = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b52872",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1f4f",
   "metadata": {},
   "source": [
    "## 6) Prequential loop (predict → update) + rule evolution tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def safe_n_rules(m):\n",
    "    \"\"\"Try multiple ways to get number of rules from different eFS implementations.\"\"\"\n",
    "    try:\n",
    "        return int(m.n_rules())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for attr in [\"n_rules\", \"rules\", \"rules_\"]:\n",
    "        if hasattr(m, attr):\n",
    "            obj = getattr(m, attr)\n",
    "            try:\n",
    "                if callable(obj):\n",
    "                    return int(obj())\n",
    "                return int(len(obj))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "def safe_metric_value(metric):\n",
    "    \"\"\"River metrics vary: some have .get(), others are float-castable.\"\"\"\n",
    "    if hasattr(metric, \"get\") and callable(metric.get):\n",
    "        return float(metric.get())\n",
    "    try:\n",
    "        return float(metric)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def safe_predict(model, x2d, default=0):\n",
    "    \"\"\"Return an int prediction, whether predict returns scalar or array-like.\"\"\"\n",
    "    try:\n",
    "        yhat = model.predict(x2d)\n",
    "        if isinstance(yhat, (list, tuple, np.ndarray)):\n",
    "            return int(yhat[0])\n",
    "        return int(yhat)\n",
    "    except Exception:\n",
    "        return int(default)\n",
    "\n",
    "def safe_online_update(model, x2d, y1d, classes=None, is_first=False):\n",
    "    \"\"\"\n",
    "    Prefer partial_fit when available.\n",
    "    Some sklearn-like APIs require `classes=` in the first call.\n",
    "    \"\"\"\n",
    "    pf = getattr(model, \"partial_fit\", None)\n",
    "    if callable(pf):\n",
    "        if is_first and classes is not None:\n",
    "            try:\n",
    "                pf(x2d, y1d, classes=classes)\n",
    "                return\n",
    "            except TypeError:\n",
    "                pass\n",
    "        pf(x2d, y1d)\n",
    "        return\n",
    "\n",
    "    # fallback\n",
    "    model.fit(x2d, y1d)\n",
    "\n",
    "\n",
    "\n",
    "# ---- streaming loop ----\n",
    "snapshots_at = {500, 1000, 1500, 2500, 3000, 4000, 5000, 6000}\n",
    "snapshots = {}\n",
    "\n",
    "acc_hist = []\n",
    "rule_count = []\n",
    "drift_points = []\n",
    "classes = np.unique(y)\n",
    "\n",
    "for t in range(N):\n",
    "    x_t = X[t]\n",
    "    y_t = int(y[t])\n",
    "\n",
    "    x2d = x_t.reshape(1, -1)\n",
    "    y1d = np.asarray([y_t])\n",
    "\n",
    "    # Predict robustly\n",
    "    y_hat = safe_predict(model, x2d, default=0)\n",
    "\n",
    "    # Update metric (in-place)\n",
    "    acc.update(y_t, y_hat)\n",
    "    acc_hist.append(safe_metric_value(acc))\n",
    "\n",
    "    # Drift detector on 0/1 error\n",
    "    err = int(y_hat != y_t)\n",
    "    out = adwin.update(err)\n",
    "\n",
    "    # River versions differ: sometimes update returns bool, sometimes flags exist\n",
    "    if (out is True) or getattr(adwin, \"drift_detected\", False) or getattr(adwin, \"change_detected\", False):\n",
    "        drift_points.append(t)\n",
    "\n",
    "    # Update model online\n",
    "    safe_online_update(model, x2d, y1d, classes=classes, is_first=(t==0))\n",
    "\n",
    "    # Track rules\n",
    "    rule_count.append(safe_n_rules(model))\n",
    "\n",
    "    # Save snapshots\n",
    "    if (t + 1) in snapshots_at:\n",
    "        snapshots[t + 1] = {\"n_rules\": rule_count[-1]}\n",
    "\n",
    "(len(drift_points), drift_points[:5], rule_count[-1] if rule_count else None, acc_hist[-1] if acc_hist else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e426ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebcd32",
   "metadata": {},
   "source": [
    "## 7) Plots: performance, drift points, and number of rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series_with_markers(y, title, ylabel, drift_points=None, drift_position=None):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    if drift_position is not None:\n",
    "        plt.axvline(drift_position, linestyle=\"--\")\n",
    "        plt.text(drift_position, plt.ylim()[1], \" true drift\", va=\"top\")\n",
    "\n",
    "    if drift_points:\n",
    "        for p in drift_points:\n",
    "            plt.axvline(p, linestyle=\":\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# True drift (from generator settings)\n",
    "TRUE_DRIFT_POS = 2500\n",
    "\n",
    "plot_series_with_markers(\n",
    "    acc_hist,\n",
    "    title=\"Prequential accuracy (predict → update)\",\n",
    "    ylabel=\"Accuracy\",\n",
    "    drift_points=drift_points,\n",
    "    drift_position=TRUE_DRIFT_POS\n",
    ")\n",
    "\n",
    "plot_series_with_markers(\n",
    "    rule_count,\n",
    "    title=\"Rule base size over time (interpretability in evolution)\",\n",
    "    ylabel=\"# rules\",\n",
    "    drift_points=drift_points,\n",
    "    drift_position=TRUE_DRIFT_POS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4a8ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052d083",
   "metadata": {},
   "source": [
    "## 8) Interpretable snapshots (rules / Gaussians / rule evolution)\n",
    "\n",
    "The following visualizations are provided by `evolvingfuzzysystems` for ENFS_Uni0:\n",
    "\n",
    "- `plot_rules()`\n",
    "- `plot_gaussians()`\n",
    "- `plot_rules_evolution()`\n",
    "\n",
    "We'll call them safely (so the notebook still runs even if a backend is missing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cce9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_call_plot(fn, title=None):\n",
    "    try:\n",
    "        if title:\n",
    "            print(title)\n",
    "        fn()\n",
    "    except Exception as e:\n",
    "        print(f\"[skip plot] {e}\")\n",
    "\n",
    "print(\"Final #rules:\", safe_n_rules(model))\n",
    "\n",
    "# Visualizations (if available in your environment)\n",
    "safe_call_plot(model.rules, title=\"Plot rules (final model)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8228d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69fb1d",
   "metadata": {},
   "source": [
    "## 9) “Explain one prediction” (local interpretability hook)\n",
    "\n",
    "A simple way to make interpretability concrete in a talk:\n",
    "1) take a single sample `x_t`  \n",
    "2) find which rule(s) fired most strongly  \n",
    "3) show their parameters / antecedent regions\n",
    "\n",
    "The ENFS_Uni0 API may not expose firing strengths directly; if not,\n",
    "you can still show:\n",
    "- nearest rule center (by Gaussian similarity)\n",
    "- which rules are currently in the knowledge base\n",
    "\n",
    "Below is a lightweight template: it tries to access internal rule storage\n",
    "and prints the first few rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-effort rule inspection (will depend on internal attributes)\n",
    "def inspect_rules(m, max_rules=3):\n",
    "    for attr in [\"rules\", \"rules_\", \"R\", \"R_\"]:\n",
    "        if hasattr(m, attr):\n",
    "            rules = getattr(m, attr)\n",
    "            try:\n",
    "                rules = rules() if callable(rules) else rules\n",
    "                print(f\"Found rules in attribute: {attr} (showing up to {max_rules})\")\n",
    "                # Print a shallow view\n",
    "                if isinstance(rules, dict):\n",
    "                    for i, (k, v) in enumerate(rules.items()):\n",
    "                        if i >= max_rules:\n",
    "                            break\n",
    "                        print(f\"- rule {k}: {str(v)[:300]}\")\n",
    "                else:\n",
    "                    for i, r in enumerate(list(rules)[:max_rules]):\n",
    "                        print(f\"- rule {i}: {str(r)[:300]}\")\n",
    "                return\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(\"Could not access internal rule storage in a stable way. Use model.plot_* instead.\")\n",
    "\n",
    "inspect_rules(model, max_rules=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d34af7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fc2a6",
   "metadata": {},
   "source": [
    "## 10) Optional extension: replace synthetic stream with a real drifting dataset\n",
    "\n",
    "Two common choices in River:\n",
    "- `datasets.Elec2()` (electricity market, drift)\n",
    "- `datasets.CreditCard()` (extreme imbalance)\n",
    "\n",
    "For your talk: Elec2 is usually “cleaner” for showing drift + adaptation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17952ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try Elec2 quickly\n",
    "real_stream = datasets.Elec2()\n",
    "x, y = next(iter(real_stream))\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039c97f",
   "metadata": {},
   "source": [
    "## 11) Paper-style comparative experiment (multiple eFS models)\n",
    "\n",
    "This section reproduces a **paper-like** comparison across several **evolving fuzzy systems** from `evolvingfuzzysystems`:\n",
    "\n",
    "- `ePL`, `ePL_plus`, `exTS`, `Simpl_eTS`, `eMG`, `ePL_KRLS_DISCO`\n",
    "- `ENFS_Uni0` (classification)\n",
    "\n",
    "**Protocol (aligned with my previous scripts):**\n",
    "- Stream → NumPy `(X, y_int)`\n",
    "- Min–max scaling of `X` to `[0, 1]` (recommended by the library)\n",
    "- **Prequential evaluation** (predict-then-update) with:\n",
    "  - Warmup (`warmup`) and then **chunked evolution** (`chunk_size`) for models that use `evolve()`\n",
    "  - For models that do not expose `evolve()`, we try `partial_fit()` or fall back to `fit()` on chunks (some evolving models implement `fit()` as an online pass).\n",
    "\n",
    "> Tip for live presentations: keep `n_max` small (e.g. 2000–5000) and use only 1–2 datasets to run fast.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Robust benchmark (hybrid) for extra-time demo\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def _ensure_1d_float(y): return np.asarray(y).ravel().astype(np.float64)\n",
    "def _ensure_1d_int(y):   return np.asarray(y).ravel().astype(int)\n",
    "\n",
    "def _predict_score_scalar(model, x_row):\n",
    "    yhat = model.predict(x_row)\n",
    "    if isinstance(yhat, (list, tuple)):\n",
    "        yhat = yhat[0] if len(yhat) else 0.0\n",
    "    arr = np.asarray(yhat)\n",
    "    return float(arr.ravel()[0]) if arr.size else 0.0\n",
    "\n",
    "def _n_rules(model):\n",
    "    if hasattr(model, \"n_rules\") and callable(getattr(model, \"n_rules\")):\n",
    "        try: return int(model.n_rules())\n",
    "        except: pass\n",
    "    for attr in [\"rules\", \"rules_\"]:\n",
    "        if hasattr(model, attr):\n",
    "            try: return int(len(getattr(model, attr)))\n",
    "            except: pass\n",
    "    return np.nan\n",
    "\n",
    "def rolling_mean(x, window):\n",
    "    x = np.asarray(x, float)\n",
    "    out = np.full_like(x, np.nan, dtype=float)\n",
    "    c = np.cumsum(np.insert(x, 0, 0.0))\n",
    "    for i in range(1, len(x)+1):\n",
    "        j0 = max(0, i-window)\n",
    "        out[i-1] = (c[i] - c[j0]) / (i - j0)\n",
    "    return out\n",
    "\n",
    "def prequential_hybrid(model, X_raw, y_int, warmup=200, batch_evolve=100, rolling_window=250, threshold=0.5):\n",
    "    X_raw = np.asarray(X_raw, dtype=np.float64)\n",
    "    y_int = _ensure_1d_int(y_int)\n",
    "    X_raw = np.nan_to_num(X_raw, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X_raw)\n",
    "\n",
    "    n = len(y_int)\n",
    "    warmup = min(warmup, n)\n",
    "\n",
    "    has_evolve = hasattr(model, \"evolve\") and callable(getattr(model, \"evolve\", None))\n",
    "    has_partial = hasattr(model, \"partial_fit\") and callable(getattr(model, \"partial_fit\", None))\n",
    "    mode = \"evolve\" if has_evolve else (\"partial_fit\" if has_partial else \"fit_batch\")\n",
    "\n",
    "    classes = np.unique(y_int)\n",
    "\n",
    "    # warmup\n",
    "    if has_evolve:\n",
    "        y0 = _ensure_1d_float(y_int[:warmup])\n",
    "        try: model.fit(X[:warmup], y0)\n",
    "        except: model.fit(X[:warmup], y0.reshape(-1,1))\n",
    "    elif has_partial:\n",
    "        for i in range(warmup):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y_int[i:i+1]\n",
    "            if i == 0:\n",
    "                try: model.partial_fit(xi, yi, classes=classes)\n",
    "                except TypeError: model.partial_fit(xi, yi)\n",
    "            else:\n",
    "                model.partial_fit(xi, yi)\n",
    "    else:\n",
    "        y0 = _ensure_1d_float(y_int[:warmup])\n",
    "        try: model.fit(X[:warmup], y0)\n",
    "        except: model.fit(X[:warmup], y0.reshape(-1,1))\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    correct_stream = []\n",
    "    acc_batch, rules_hist = [], []\n",
    "    bufX, bufy = [], []\n",
    "\n",
    "    for t in range(warmup, n):\n",
    "        x_t = X[t:t+1]\n",
    "        y_t = int(y_int[t])\n",
    "\n",
    "        score = _predict_score_scalar(model, x_t)\n",
    "        y_hat = int(score >= threshold)\n",
    "\n",
    "        y_true.append(y_t); y_pred.append(y_hat)\n",
    "        correct_stream.append(1.0 if y_hat == y_t else 0.0)\n",
    "\n",
    "        if mode == \"evolve\":\n",
    "            bufX.append(x_t.ravel()); bufy.append(float(y_int[t]))\n",
    "            if len(bufy) >= batch_evolve:\n",
    "                Xb = np.vstack(bufX)\n",
    "                yb = _ensure_1d_float(np.array(bufy))\n",
    "                try: model.evolve(Xb, yb)\n",
    "                except: model.evolve(Xb, yb.reshape(-1,1))\n",
    "                bufX, bufy = [], []\n",
    "        elif mode == \"partial_fit\":\n",
    "            model.partial_fit(x_t, np.array([y_t], dtype=int))\n",
    "        else:\n",
    "            bufX.append(x_t.ravel()); bufy.append(float(y_int[t]))\n",
    "            if len(bufy) >= batch_evolve:\n",
    "                Xb = np.vstack(bufX)\n",
    "                yb = _ensure_1d_float(np.array(bufy))\n",
    "                try: model.fit(Xb, yb)\n",
    "                except: model.fit(Xb, yb.reshape(-1,1))\n",
    "                bufX, bufy = [], []\n",
    "\n",
    "        if (t - warmup + 1) % batch_evolve == 0:\n",
    "            acc_batch.append(accuracy_score(np.asarray(y_true), np.asarray(y_pred)))\n",
    "            rules_hist.append(_n_rules(model))\n",
    "\n",
    "    correct_stream = np.asarray(correct_stream, float)\n",
    "    acc_roll = rolling_mean(correct_stream, rolling_window)\n",
    "\n",
    "    return {\"acc_batch\": np.asarray(acc_batch),\n",
    "            \"acc_roll\": acc_roll,\n",
    "            \"rules\": np.asarray(rules_hist),\n",
    "            \"correct_stream\": correct_stream,\n",
    "            \"mode\": mode}\n",
    "\n",
    "# Example: quick extra benchmark on Elec2 arrays (Xr, yr)\n",
    "warmup = 500\n",
    "batch_evolve = 100\n",
    "rolling_window = 400\n",
    "threshold = 0.5\n",
    "\n",
    "MODELS_REAL = [\n",
    "    (\"ENFS_Uni0\", lambda: ENFS_Uni0(n_features=X_raw.shape[1], n_classes=len(np.unique(y_raw)),\n",
    "                                   lambda_ff=0.99, sim_threshold=0.95, max_rules=10, random_state=42)),\n",
    "    (\"ePL\", lambda: ePL()),\n",
    "    (\"exTS\", lambda: exTS()),\n",
    "    (\"eMG\", lambda: eMG()),\n",
    "]\n",
    "\n",
    "results_real = {}\n",
    "rows = []\n",
    "failed = []\n",
    "\n",
    "for name, ctor in MODELS_REAL:\n",
    "    print(\"Running:\", name)\n",
    "    t0 = perf_counter()\n",
    "    try:\n",
    "        res = prequential_hybrid(ctor(), X_raw, y_raw, warmup=warmup, batch_evolve=batch_evolve,\n",
    "                                rolling_window=rolling_window, threshold=threshold)\n",
    "        dt = perf_counter() - t0\n",
    "        results_real[name] = res\n",
    "        rows.append({\"model\": name, \"mode\": res[\"mode\"],\n",
    "                     \"final_acc_batch\": float(res[\"acc_batch\"][-1]) if len(res[\"acc_batch\"]) else np.nan,\n",
    "                     \"mean_acc_batch\": float(np.mean(res[\"acc_batch\"])) if len(res[\"acc_batch\"]) else np.nan,\n",
    "                     \"final_rules\": float(res[\"rules\"][-1]) if len(res[\"rules\"]) else np.nan,\n",
    "                     \"runtime_s\": float(dt)})\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {name} -> {repr(e)} (skipping)\")\n",
    "        failed.append(name)\n",
    "\n",
    "df_extra = pd.DataFrame(rows).sort_values(\"final_acc_batch\", ascending=False)\n",
    "print(\"Failed:\", failed)\n",
    "df_extra\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
