{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2492d1f",
   "metadata": {},
   "source": [
    "# Fuzzy Neural Networks (FNN) — Notebook 1  \n",
    "### Grid-partition fuzzification (Gaussian MFs), rule explosion, and interpretability (using your `FNNModel`)\n",
    "\n",
    "**Seminar (Day 1)** — Universidad Carlos III de Madrid  \n",
    "**Instructor:** Prof. Doutor Paulo Vitor de Campos Souza  \n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook is for\n",
    "\n",
    "This is a *presentation notebook*: every executable cell produces an output (a plot and/or short printed summary),\n",
    "so you can keep a dynamic pace during the seminar.\n",
    "\n",
    "We will walk through your FNN as a **3-layer architecture**:\n",
    "\n",
    "1) **Layer 1 — Fuzzification (Grid Partition):** Gaussian membership functions create *Gaussian neurons*  \n",
    "   and (optionally) feature-level weights can be adjusted by **data density** for interpretability.\n",
    "\n",
    "2) **Layer 2 — Logic / Rule Layer:** AND / OR neurons aggregate the MF activations into **fuzzy rules**.  \n",
    "   The rule consequents (output weights) are estimated using **Moore–Penrose pseudo-inverse**  \n",
    "   (we mention PSO / other optimizers as research extensions).\n",
    "\n",
    "3) **Layer 3 — Output / Defuzzification:** a **singleton** output produces the final crisp prediction.\n",
    "\n",
    "Finally, we compute interpretability diagnostics such as:\n",
    "- **Consistency** (between consequents),\n",
    "- **Similarity** (between rule parameters),\n",
    "- **Distinguishability / overlap** (between antecedent Gaussians),\n",
    "- **Completeness** (coverage of samples by rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494f021",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0) Imports and a note about project structure\n",
    "\n",
    "In your full project, the model is typically imported as:\n",
    "\n",
    "```python\n",
    "from models.models import FNNModel\n",
    "```\n",
    "\n",
    "(as in your experiment runner script).",
    "\n",
    "This notebook tries that first. If you are running from a single folder (only `models.py` available),\n",
    "it will automatically fall back to a local import.\n",
    "\n",
    "> For the UC3M seminar zip, I recommend shipping the full folder structure so the first import works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ecd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# --- Load FNNModel (preferred path: full project) ---\n",
    "try:\n",
    "    from models.models import FNNModel\n",
    "    print(\"✓ Imported FNNModel from full project path: models.models.FNNModel\")\n",
    "except Exception as e_full:\n",
    "    print(\"Full-project import failed:\", repr(e_full))\n",
    "    print(\"Trying local fallback import from ./models.py ...\")\n",
    "    import importlib.util, pathlib\n",
    "    p = pathlib.Path(\"./models.py\")\n",
    "    if not p.exists():\n",
    "        # also try the sandbox location if needed\n",
    "        p = pathlib.Path(\"/mnt/data/models.py\")\n",
    "    spec = importlib.util.spec_from_file_location(\"models_local\", p)\n",
    "    models_local = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(models_local)\n",
    "    FNNModel = models_local.FNNModel\n",
    "    print(f\"✓ Imported FNNModel from: {p}\")\n",
    "\n",
    "print(\"FNNModel:\", FNNModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966aa64c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Dataset: Iris (binary)\n",
    "\n",
    "Your `FNNModel` evaluation uses `sign()` and expects **binary labels** in **{-1, +1}**.\n",
    "For a clean, well-known demo we turn Iris into a binary problem:\n",
    "\n",
    "- +1: *setosa*\n",
    "- -1: *versicolor*\n",
    "(we ignore *virginica* in this notebook)\n",
    "\n",
    "This is perfect to visualize fuzzification and rules because we have **4 input features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X_all = iris.data\n",
    "y_all = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "mask = (y_all == 0) | (y_all == 1)   # setosa vs versicolor\n",
    "X = X_all[mask]\n",
    "y_raw = y_all[mask]\n",
    "y = np.where(y_raw == 0, 1, -1).astype(int)\n",
    "\n",
    "# Normalize (recommended for Gaussian MFs)\n",
    "scaler = StandardScaler()\n",
    "Xn = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"✓ Data ready\")\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"Classes:\", {+1: int((y==1).sum()), -1: int((y==-1).sum())})\n",
    "print(\"Features:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1337875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Build DataFrame for analysis\n",
    "# --------------------------------------------------\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df[\"class\"] = np.where(y == 1, \"Setosa (+1)\", \"Versicolor (-1)\")\n",
    "\n",
    "print(\"✓ DataFrame created\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2691d",
   "metadata": {},
   "source": [
    "This statistical analysis provides a quantitative justification for the placement and spread of membership functions. Features with higher inter-class separation tend to generate more discriminative fuzzy rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b081449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 2) Descriptive statistics per class\n",
    "# --------------------------------------------------\n",
    "stats = []\n",
    "\n",
    "for feat in feature_names:\n",
    "    x_pos = df[df[\"class\"] == \"Setosa (+1)\"][feat].values\n",
    "    x_neg = df[df[\"class\"] == \"Versicolor (-1)\"][feat].values\n",
    "\n",
    "    mean_pos, mean_neg = x_pos.mean(), x_neg.mean()\n",
    "    std_pos, std_neg = x_pos.std(ddof=1), x_neg.std(ddof=1)\n",
    "    med_pos, med_neg = np.median(x_pos), np.median(x_neg)\n",
    "\n",
    "    # Effect size: Cohen's d\n",
    "    pooled_std = np.sqrt((std_pos**2 + std_neg**2) / 2)\n",
    "    cohens_d = (mean_pos - mean_neg) / pooled_std if pooled_std > 0 else 0.0\n",
    "\n",
    "    # Welch t-test (robust to unequal variances)\n",
    "    t_stat, p_val = ttest_ind(x_pos, x_neg, equal_var=False)\n",
    "\n",
    "    stats.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Mean (+1)\": mean_pos,\n",
    "        \"Mean (-1)\": mean_neg,\n",
    "        \"Std (+1)\": std_pos,\n",
    "        \"Std (-1)\": std_neg,\n",
    "        \"Median (+1)\": med_pos,\n",
    "        \"Median (-1)\": med_neg,\n",
    "        \"Δ Mean\": mean_pos - mean_neg,\n",
    "        \"Cohen's d\": cohens_d,\n",
    "        \"p-value (Welch)\": p_val\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 3) Highlight statistically relevant features (fixed)\n",
    "# --------------------------------------------------\n",
    "\n",
    "stats_df2 = stats_df.copy()\n",
    "\n",
    "# Choose ONE magnitude column to sort by:\n",
    "# Option A (recommended): absolute effect size\n",
    "stats_df2[\"|Cohen's d|\"] = stats_df2[\"Cohen's d\"].abs()\n",
    "\n",
    "# Option B: absolute mean difference (also useful)\n",
    "stats_df2[\"|Δ Mean|\"] = stats_df2[\"Δ Mean\"].abs()\n",
    "\n",
    "# Sort by p-value first, then by effect size magnitude\n",
    "stats_df_sorted = stats_df2.sort_values(\n",
    "    by=[\"p-value (Welch)\", \"|Cohen's d|\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "print(\"✓ Features ranked by statistical separation (Welch test + effect size):\")\n",
    "stats_df_sorted[[\"Feature\", \"Δ Mean\", \"|Δ Mean|\", \"Cohen's d\", \"|Cohen's d|\", \"p-value (Welch)\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4) Visualization: boxplots + KDEs\n",
    "# --------------------------------------------------\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "for feat in feature_names:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"class\",\n",
    "        y=feat,\n",
    "        hue=\"class\",\n",
    "        palette={\"Setosa (+1)\": \"#4CAF50\", \"Versicolor (-1)\": \"#FF9800\"},\n",
    "        legend=False,\n",
    "        ax=axes[0]\n",
    ")\n",
    "    # KDE\n",
    "    sns.kdeplot(\n",
    "        data=df, x=feat, hue=\"class\",\n",
    "        fill=True, common_norm=False,\n",
    "        alpha=0.5, ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f\"Density (KDE) — {feat}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc3975",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) The core knobs\n",
    "\n",
    "These parameters are the easiest way to show the main effects:\n",
    "\n",
    "- `NUM_MFS`: 2 → 3 → 4 (watch the rule explosion)\n",
    "- `NEURON_TYPE`: AND / OR / UNINORM\n",
    "- `OPTIMIZER`: Moore–Penrose (fast & didactic), plus PSO/Adam as research options\n",
    "- `PRUNING`: keep `\"none\"` for the seminar baseline\n",
    "\n",
    "> Tip: During the talk, change only **one** knob at a time so the audience can attribute the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MFS = 2                 # try: 2, 3, 4\n",
    "NEURON_TYPE = \"andneuron\"   # \"andneuron\" | \"orneuron\" |\n",
    "ACTIVATION = \"linear\"       # best match for pseudo-inverse explanation\n",
    "OPTIMIZER = \"moore-penrose\" # \"moore-penrose\" | \"pso\" | \"adam\"\n",
    "PRUNING = \"none\"            # keep 'none' for the seminar baseline\n",
    "\n",
    "print(\"✓ Knobs set\")\n",
    "print(dict(NUM_MFS=NUM_MFS, NEURON_TYPE=NEURON_TYPE, ACTIVATION=ACTIVATION, OPTIMIZER=OPTIMIZER, PRUNING=PRUNING))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1c2d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Layer 1 — Fuzzification (Gaussian MFs, grid partition)\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "- For each feature, we define `NUM_MFS` membership functions (MFs) using a **grid partition** strategy.\n",
    "- In your implementation, the MFs are **Gaussians**, producing *Gaussian neurons*.\n",
    "- Each sample is transformed into a set of MF activation values (degrees of membership).\n",
    "\n",
    "If `visualizeMF=True`, the model will plot the Gaussian MFs for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNNModel(\n",
    "    num_mfs=NUM_MFS,\n",
    "    neuron_type=NEURON_TYPE,\n",
    "    activation=ACTIVATION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    pruning=PRUNING,\n",
    "    visualizeMF=True,\n",
    "    rng_seed=np.random.default_rng(42),\n",
    ")\n",
    "\n",
    "F = model.fuzzification_layer(X_train)\n",
    "\n",
    "print(\"✓ Fuzzification completed\")\n",
    "print(\"F shape (MF activations):\", F.shape)\n",
    "print(\"MF activations per sample should be num_mfs * num_features =\", NUM_MFS * X_train.shape[1])\n",
    "print(\"num_features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2d9f8",
   "metadata": {},
   "source": [
    "### Rule explosion (a key seminar moment)\n",
    "\n",
    "With `d` input features and `m` membership functions per feature:\n",
    "\n",
    "\\[\n",
    "\\text{#rules} = m^d\n",
    "\\]\n",
    "\n",
    "For Iris (d=4):\n",
    "- 2 MFs → 16 rules\n",
    "- 3 MFs → 81 rules\n",
    "- 4 MFs → 256 rules\n",
    "\n",
    "This is why interpretability often involves:\n",
    "- pruning / merging,\n",
    "- or evolving strategies to control complexity (Day 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce41320",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train.shape[1]\n",
    "for m in [2, 3, 4, 5, 6]:\n",
    "    print(f\"num_mfs={m:>2}  ->  rules = {m**d:>6}\")\n",
    "print(\"✓ Rule explosion printed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170328b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Gaussian vs Triangular membership functions (live visual comparison)\n",
    "\n",
    "Your model uses **Gaussian** MFs.  \n",
    "A natural research question is: *what happens if we replace the fuzzification with triangular MFs?*\n",
    "\n",
    "- Gaussian: smooth tails, differentiable, very common in neuro-fuzzy models  \n",
    "- Triangular: piecewise-linear, often perceived as more “linguistic” / interpretable\n",
    "\n",
    "Below we **do not modify the model** (we keep training Gaussian),\n",
    "but we plot Gaussian vs Triangular side-by-side using the **same centers** learned during fuzzification.\n",
    "This gives a very clear visual for the audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def triangular_mf(x, a, b, c):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.zeros_like(x)\n",
    "    left = (a < x) & (x <= b)\n",
    "    y[left] = (x[left] - a) / (b - a + 1e-12)\n",
    "    right = (b < x) & (x < c)\n",
    "    y[right] = (c - x[right]) / (c - b + 1e-12)\n",
    "    y[x == b] = 1.0\n",
    "    return np.clip(y, 0, 1)\n",
    "\n",
    "# Choose a feature to visualize\n",
    "feat = 0  # change to 1,2,3 during the talk\n",
    "\n",
    "centers = np.array(model.mf_params[feat][\"centers\"], dtype=float)\n",
    "sigmas  = np.array(model.mf_params[feat][\"sigmas\"], dtype=float)\n",
    "\n",
    "xcol = X_train[:, feat]\n",
    "xs = np.linspace(xcol.min(), xcol.max(), 600)\n",
    "\n",
    "# Build triangles with half-width proportional to sigma (demo choice)\n",
    "k = 2.0\n",
    "tri_params = [(c - k*s, c, c + k*s) for c, s in zip(centers, sigmas)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "# Gaussian MFs\n",
    "for c, s in zip(centers, sigmas):\n",
    "    axes[0].plot(xs, norm.pdf(xs, c, s))\n",
    "axes[0].set_title(f\"Gaussian MFs (feature {feat+1})\")\n",
    "axes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"membership (pdf)\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Triangular MFs\n",
    "for (a, b, c) in tri_params:\n",
    "    axes[1].plot(xs, triangular_mf(xs, a, b, c))\n",
    "axes[1].set_title(f\"Triangular MFs (feature {feat+1})\")\n",
    "axes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"membership (degree)\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gaussian vs Triangular plotted\")\n",
    "print(\"Centers:\", np.round(centers, 3))\n",
    "print(\"Sigmas :\", np.round(sigmas, 3))\n",
    "print(\"Triangle half-width multiplier k =\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f32a6c",
   "metadata": {},
   "source": [
    "The decision on which form of fuzzification to choose for your model can determine complexity in several aspects, including interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load Iris and pick 2 features\n",
    "# ---------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Pick 2 features for visualization (edit if you want)\n",
    "f1, f2 = 2, 3  # petal length, petal width (usually very separable)\n",
    "X2 = X[:, [f1, f2]]\n",
    "\n",
    "# Standardize for clustering stability (recommended)\n",
    "scaler = StandardScaler()\n",
    "X2s = scaler.fit_transform(X2)\n",
    "\n",
    "print(\"✓ Iris loaded:\", X2.shape)\n",
    "print(\"Features:\", feature_names[f1], \"and\", feature_names[f2])\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Helpers: Gaussian MF\n",
    "# ---------------------------\n",
    "def gaussian_mf(x, c, s):\n",
    "    s = max(float(s), 1e-9)\n",
    "    return np.exp(-0.5 * ((x - c) / s) ** 2)\n",
    "\n",
    "def grid_centers_sigmas(x, m=3):\n",
    "    # evenly spaced centers across data range\n",
    "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
    "    centers = np.linspace(xmin, xmax, m)\n",
    "    # sigma: spacing-based (simple heuristic)\n",
    "    if m > 1:\n",
    "        spacing = centers[1] - centers[0]\n",
    "        sigma = spacing / 2.0\n",
    "    else:\n",
    "        sigma = (xmax - xmin) / 2.0 if xmax > xmin else 1.0\n",
    "    sigmas = np.full_like(centers, sigma, dtype=float)\n",
    "    return centers, sigmas\n",
    "\n",
    "def cluster_centers_sigmas(x, k=3, random_state=0):\n",
    "    # 1D KMeans on a single feature\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "    km.fit(x.reshape(-1, 1))\n",
    "    centers = np.sort(km.cluster_centers_.ravel())\n",
    "    # sigma: average distance to nearest center (heuristic)\n",
    "    if k > 1:\n",
    "        spacing = np.mean(np.diff(centers))\n",
    "        sigma = spacing / 2.0\n",
    "    else:\n",
    "        sigma = np.std(x) if np.std(x) > 0 else 1.0\n",
    "    sigmas = np.full_like(centers, sigma, dtype=float)\n",
    "    return centers, sigmas\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Build MFs for both features\n",
    "# ---------------------------\n",
    "m = 3  # number of MFs / clusters (edit: 2,3,4)\n",
    "\n",
    "x1 = X2[:, 0]\n",
    "x2 = X2[:, 1]\n",
    "\n",
    "# Grid\n",
    "c1g, s1g = grid_centers_sigmas(x1, m=m)\n",
    "c2g, s2g = grid_centers_sigmas(x2, m=m)\n",
    "\n",
    "# Cluster-based (use standardized for clustering, but plot in original scale)\n",
    "c1c_s, s1c_s = cluster_centers_sigmas(X2s[:, 0], k=m, random_state=0)\n",
    "c2c_s, s2c_s = cluster_centers_sigmas(X2s[:, 1], k=m, random_state=0)\n",
    "\n",
    "# Convert cluster centers back to original scale for plotting MFs\n",
    "c1c = scaler.mean_[0] + c1c_s * scaler.scale_[0]\n",
    "c2c = scaler.mean_[1] + c2c_s * scaler.scale_[1]\n",
    "# Sigmas back to original scale\n",
    "s1c = s1c_s * scaler.scale_[0]\n",
    "s2c = s2c_s * scaler.scale_[1]\n",
    "\n",
    "print(\"✓ Grid centers (feature 1):\", np.round(c1g, 2))\n",
    "print(\"✓ Cluster centers (feature 1):\", np.round(c1c, 2))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Plot 1D fuzzification side-by-side\n",
    "# ---------------------------\n",
    "xx1 = np.linspace(np.min(x1), np.max(x1), 400)\n",
    "xx2 = np.linspace(np.min(x2), np.max(x2), 400)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 7), constrained_layout=True)\n",
    "\n",
    "# Feature 1: Grid vs Cluster\n",
    "axes[0, 0].set_title(f\"Grid fuzzification — {feature_names[f1]}\")\n",
    "for c, s in zip(c1g, s1g):\n",
    "    axes[0, 0].plot(xx1, gaussian_mf(xx1, c, s))\n",
    "axes[0, 0].set_xlabel(feature_names[f1]); axes[0, 0].set_ylabel(\"μ\")\n",
    "\n",
    "axes[0, 1].set_title(f\"Cluster-based fuzzification — {feature_names[f1]}\")\n",
    "for c, s in zip(c1c, s1c):\n",
    "    axes[0, 1].plot(xx1, gaussian_mf(xx1, c, s))\n",
    "axes[0, 1].set_xlabel(feature_names[f1]); axes[0, 1].set_ylabel(\"μ\")\n",
    "\n",
    "# Feature 2: Grid vs Cluster\n",
    "axes[1, 0].set_title(f\"Grid fuzzification — {feature_names[f2]}\")\n",
    "for c, s in zip(c2g, s2g):\n",
    "    axes[1, 0].plot(xx2, gaussian_mf(xx2, c, s))\n",
    "axes[1, 0].set_xlabel(feature_names[f2]); axes[1, 0].set_ylabel(\"μ\")\n",
    "\n",
    "axes[1, 1].set_title(f\"Cluster-based fuzzification — {feature_names[f2]}\")\n",
    "for c, s in zip(c2c, s2c):\n",
    "    axes[1, 1].plot(xx2, gaussian_mf(xx2, c, s))\n",
    "axes[1, 1].set_xlabel(feature_names[f2]); axes[1, 1].set_ylabel(\"μ\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 5) (Optional, very nice) 2D view: show MF \"regions\"\n",
    "# Pick one MF index to visualize as a contour\n",
    "# ---------------------------\n",
    "mf_idx = 1  # 0..m-1\n",
    "grid_res = 120\n",
    "\n",
    "x1_grid = np.linspace(np.min(x1), np.max(x1), grid_res)\n",
    "x2_grid = np.linspace(np.min(x2), np.max(x2), grid_res)\n",
    "Xg1, Xg2 = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "# Example: product of membership degrees in 2D (AND-like)\n",
    "Z_grid = gaussian_mf(Xg1, c1g[mf_idx], s1g[mf_idx]) * gaussian_mf(Xg2, c2g[mf_idx], s2g[mf_idx])\n",
    "Z_clst = gaussian_mf(Xg1, c1c[mf_idx], s1c[mf_idx]) * gaussian_mf(Xg2, c2c[mf_idx], s2c[mf_idx])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "\n",
    "axes[0].set_title(f\"2D MF region (Grid) — MF{mf_idx+1}\")\n",
    "axes[0].scatter(x1, x2, c=y, s=18, alpha=0.7)\n",
    "cs0 = axes[0].contour(Xg1, Xg2, Z_grid, levels=6)\n",
    "axes[0].clabel(cs0, inline=True, fontsize=8)\n",
    "axes[0].set_xlabel(feature_names[f1]); axes[0].set_ylabel(feature_names[f2])\n",
    "\n",
    "axes[1].set_title(f\"2D MF region (Cluster-based) — MF{mf_idx+1}\")\n",
    "axes[1].scatter(x1, x2, c=y, s=18, alpha=0.7)\n",
    "cs1 = axes[1].contour(Xg1, Xg2, Z_clst, levels=6)\n",
    "axes[1].clabel(cs1, inline=True, fontsize=8)\n",
    "axes[1].set_xlabel(feature_names[f1]); axes[1].set_ylabel(feature_names[f2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Done: Grid vs Cluster fuzzification visual comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e77bfa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Layer 2 — Logic neurons ⇒ fuzzy rules (AND / OR )\n",
    "\n",
    "In your architecture, the second layer aggregates MF activations into **rule activations** using a logic neuron:\n",
    "\n",
    "- AND neuron: tends to be more “selective” (requires all antecedents to be high)\n",
    "- OR neuron: tends to be more “inclusive”\n",
    "\n",
    "Your implementation supports those options via `neuron_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d14c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.logic_neurons_layer(F)\n",
    "print(\"✓ Logic / rule layer computed\")\n",
    "print(\"A shape (rule activations):\", A.shape)\n",
    "print(\"Expected #rules:\", NUM_MFS ** X_train.shape[1])\n",
    "print(\"Rule activation stats: min={:.3e}, mean={:.3e}, max={:.3e}\".format(A.min(), A.mean(), A.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595a19e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6) Rule consequents via Moore–Penrose pseudo-inverse (training)\n",
    "\n",
    "In your pipeline, once the rule activations are computed, the rule consequents (output weights) are estimated.\n",
    "For the seminar we focus on the **Moore–Penrose pseudo-inverse** because it is:\n",
    "\n",
    "- fast,\n",
    "- closed-form,\n",
    "- very easy to explain as “linear regression on rule features”.\n",
    "\n",
    "You also support other optimizers (PSO/Adam) as active research directions (mentioned but not executed live). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(X_train, y_train)\n",
    "y_pred, metrics = model.evaluate_model(X_test, y_test)\n",
    "\n",
    "print(\"✓ Training + evaluation done\")\n",
    "print(\"Metrics:\", metrics)\n",
    "print(\"V shape (rule consequents / output weights):\", np.asarray(model.V).shape)\n",
    "print(\"Pred sample:\", y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6703c37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7) Interpretable fuzzy rules (text and image)\n",
    "\n",
    "A key advantage of your approach is that it can *export* fuzzy rules in a readable form.\n",
    "In the seminar, it is usually enough to show the first ~5 rules and discuss:\n",
    "\n",
    "- the MF index per feature,\n",
    "- the rule consequent sign/magnitude,\n",
    "- and how this ties back to the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = model.generate_fuzzy_rules()\n",
    "print(\"✓ Rules generated:\", len(rules))\n",
    "for r in rules[:16]:\n",
    "    print(\"-\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d608bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Customize labels (edit here)\n",
    "# -----------------------------\n",
    "FEATURE_NAMES = [\n",
    "    \"sepal length\",\n",
    "    \"sepal width\",\n",
    "    \"petal length\",\n",
    "    \"petal width\",\n",
    "]\n",
    "\n",
    "MF_LABELS = {\n",
    "    1: \"small\",\n",
    "    2: \"medium\",\n",
    "    3: \"large\",\n",
    "    4: \"very large\",\n",
    "}\n",
    "\n",
    "def format_consequent(v):\n",
    "    v = float(v)\n",
    "    cls = \"+1\" if v >= 0 else \"-1\"\n",
    "    return f\"{v:.4f} (class {cls})\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Robust access to model sizes / weights / consequents\n",
    "# -------------------------------------------------------\n",
    "n_features = len(getattr(model, \"mf_params\", FEATURE_NAMES))\n",
    "\n",
    "# number of rules as used by the model when printing rules\n",
    "n_rules = int(getattr(model, \"Oldtotal_fuzzy_neurons\", getattr(model, \"total_fuzzy_neurons\", 0)))\n",
    "if n_rules == 0:\n",
    "    # fallback: derive from num_mfs^d\n",
    "    n_rules = int(model.num_mfs ** n_features)\n",
    "\n",
    "# impacts\n",
    "W = getattr(model, \"neuron_weights\", None)\n",
    "if W is None:\n",
    "    print(\"⚠️ model.neuron_weights not found. Impacts will be set to 1.00\")\n",
    "    W = np.ones((n_rules, n_features), dtype=float)\n",
    "else:\n",
    "    W = np.asarray(W, dtype=float)\n",
    "\n",
    "# consequents: your code sometimes stores them as OldV\n",
    "V_raw = getattr(model, \"OldV\", getattr(model, \"V\", None))\n",
    "if V_raw is None:\n",
    "    raise AttributeError(\"Could not find model.V or model.OldV for consequents.\")\n",
    "V_raw = np.asarray(V_raw, dtype=float)\n",
    "if V_raw.ndim > 1:\n",
    "    V_raw = V_raw[:, 0]  # use first column by default\n",
    "\n",
    "print(\"✓ Using:\")\n",
    "print(\"  n_features:\", n_features)\n",
    "print(\"  n_rules   :\", n_rules)\n",
    "print(\"  W shape   :\", W.shape)\n",
    "print(\"  V shape   :\", V_raw.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3) Human-readable rules WITH impacts\n",
    "# ------------------------------------------\n",
    "def build_readable_rules_with_impacts(model, top_k=10):\n",
    "    out = []\n",
    "    dims = [model.num_mfs] * n_features\n",
    "\n",
    "    for rule_index in range(min(top_k, n_rules)):\n",
    "        mf_combination = np.unravel_index(rule_index, dims)  # tuple of 0-based MF indices\n",
    "\n",
    "        parts = []\n",
    "        for feat_idx, mf_idx0 in enumerate(mf_combination):\n",
    "            mf_idx = int(mf_idx0) + 1  # 1-based (MF1..)\n",
    "            term = MF_LABELS.get(mf_idx, f\"MF{mf_idx}\")\n",
    "\n",
    "            feat_name = FEATURE_NAMES[feat_idx] if feat_idx < len(FEATURE_NAMES) else f\"x{feat_idx+1}\"\n",
    "            impact = float(W[rule_index, feat_idx]) if (rule_index < W.shape[0] and feat_idx < W.shape[1]) else 1.0\n",
    "\n",
    "            parts.append(f\"{feat_name} is {term} (MF{mf_idx}) with impact {impact:.2f}\")\n",
    "\n",
    "        # AND/OR text (keep consistent with your model behavior)\n",
    "        if getattr(model, \"neuron_type\", \"\") == \"unineuron\":\n",
    "            # if you store per-rule operation_type in neuron_details, use it; else default AND\n",
    "            op = \"AND\"\n",
    "            try:\n",
    "                op = model.neuron_details[rule_index].get(\"operation_type\", \"AND\")\n",
    "            except Exception:\n",
    "                op = \"AND\"\n",
    "            joiner = f\" {op} \"\n",
    "        else:\n",
    "            joiner = \" AND \" if getattr(model, \"neuron_type\", \"\") == \"andneuron\" else \" OR \"\n",
    "\n",
    "        antecedent = joiner.join(parts)\n",
    "        consequent = format_consequent(V_raw[rule_index])\n",
    "\n",
    "        out.append(f\"Rule {rule_index+1:02d}: IF {antecedent} THEN output = {consequent}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "readable_rules = build_readable_rules_with_impacts(model, top_k=16)\n",
    "\n",
    "print(\"\\n=== Human-readable fuzzy rules (with impacts) ===\")\n",
    "for r in readable_rules:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_to_kg_object(\n",
    "    model,\n",
    "    rule_index,\n",
    "    feature_names,\n",
    "    mf_labels,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert one fuzzy rule from the model into a KG-friendly dictionary.\n",
    "    \"\"\"\n",
    "    n_features = len(feature_names)\n",
    "    dims = [model.num_mfs] * n_features\n",
    "\n",
    "    # antecedent MF combination (same logic as your rule generator)\n",
    "    mf_combination = np.unravel_index(rule_index, dims)\n",
    "\n",
    "    antecedents = []\n",
    "    for feat_idx, mf_idx0 in enumerate(mf_combination):\n",
    "        mf_idx = mf_idx0 + 1\n",
    "        antecedents.append({\n",
    "            \"feature\": feature_names[feat_idx],\n",
    "            \"mf_label\": mf_labels.get(mf_idx, f\"MF{mf_idx}\"),\n",
    "            \"mf_index\": mf_idx,\n",
    "            \"impact\": float(model.neuron_weights[rule_index, feat_idx]),\n",
    "        })\n",
    "\n",
    "    # consequent\n",
    "    V = np.asarray(getattr(model, \"OldV\", getattr(model, \"V\")))\n",
    "    if V.ndim > 1:\n",
    "        V = V[:, 0]\n",
    "\n",
    "    consequent = {\n",
    "        \"raw_value\": float(V[rule_index]),\n",
    "        \"class\": \"+1\" if V[rule_index] >= 0 else \"-1\",\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"id\": f\"Rule_{rule_index+1:03d}\",\n",
    "        \"type\": \"FuzzyRule\",\n",
    "        \"antecedents\": antecedents,\n",
    "        \"consequent\": consequent,\n",
    "    }\n",
    "kg_rule = rule_to_kg_object(\n",
    "    model=model,\n",
    "    rule_index=2,  # Rule 3\n",
    "    feature_names=FEATURE_NAMES,\n",
    "    mf_labels=MF_LABELS,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(kg_rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67334f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# -----------------------------\n",
    "# Customize (edit here)\n",
    "# -----------------------------\n",
    "FEATURE_NAMES = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "\n",
    "MF_LABELS = {1: \"small\", 2: \"medium\", 3: \"large\", 4: \"very large\"}\n",
    "\n",
    "# Class labels for your seminar legend (edit as needed)\n",
    "# NOTE: In the notebook we used setosa vs versicolor. If you want virginica, change here.\n",
    "CLASS_NAME = {+1: \"Class +1 (Setosa)\", -1: \"Class -1 (Versicolor)\"}\n",
    "\n",
    "# Output node colors (requested: green vs orange)\n",
    "CLASS_COLOR = {+1: \"#5543b4\", -1: \"#ba5970\"}  # green / orange\n",
    "\n",
    "# Colormap for input nodes based on impact\n",
    "IMPACT_CMAP = cm.Reds  # higher impact => redder\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def safe_get_V(model):\n",
    "    V = np.asarray(getattr(model, \"OldV\", getattr(model, \"V\", None)), dtype=float)\n",
    "    if V.ndim > 1:\n",
    "        V = V[:, 0]\n",
    "    return V\n",
    "\n",
    "def safe_get_W(model):\n",
    "    W = getattr(model, \"neuron_weights\", None)\n",
    "    if W is None:\n",
    "        raise ValueError(\"model.neuron_weights not found. Train the model first.\")\n",
    "    return np.asarray(W, dtype=float)\n",
    "\n",
    "def rule_mf_combo(model, rule_index, n_features):\n",
    "    dims = [model.num_mfs] * n_features\n",
    "    return np.unravel_index(rule_index, dims)  # tuple of 0-based MF indices\n",
    "\n",
    "def draw_one_rule(ax, model, rule_index, V, W, norm):\n",
    "    n_features = len(FEATURE_NAMES)\n",
    "    mf_combo = rule_mf_combo(model, rule_index, n_features)\n",
    "\n",
    "    # Determine class by sign of V\n",
    "    v = float(V[rule_index])\n",
    "    cls = +1 if v >= 0 else -1\n",
    "\n",
    "    # Build small graph\n",
    "    G = nx.DiGraph()\n",
    "    rule_node = f\"Rule_{rule_index+1}\"\n",
    "    out_node = f\"Output_{rule_index+1}\"\n",
    "\n",
    "    G.add_node(rule_node, kind=\"rule\", label=f\"Rule {rule_index+1}\")\n",
    "    G.add_node(out_node, kind=\"output\", label=f\"{CLASS_NAME[cls]}\")\n",
    "\n",
    "    # Antecedents\n",
    "    impacts = []\n",
    "    for j in range(n_features):\n",
    "        mf_idx = int(mf_combo[j]) + 1\n",
    "        term = MF_LABELS.get(mf_idx, f\"MF{mf_idx}\")\n",
    "        impact = float(W[rule_index, j])\n",
    "        impacts.append(impact)\n",
    "\n",
    "        ant_node = f\"A{j+1}\"\n",
    "        ant_label = f\"{FEATURE_NAMES[j]}\\n= {term} (MF{mf_idx})\\nimpact={impact:.2f}\"\n",
    "        G.add_node(ant_node, kind=\"antecedent\", label=ant_label, impact=impact)\n",
    "\n",
    "        G.add_edge(ant_node, rule_node, weight=impact)\n",
    "\n",
    "    G.add_edge(rule_node, out_node, weight=1.0)\n",
    "\n",
    "    # Layout: inputs (x=0), rule (x=1.2), output (x=2.4)\n",
    "    pos = {}\n",
    "    y_positions = np.linspace(1, 0, n_features)\n",
    "    for j in range(n_features):\n",
    "        pos[f\"A{j+1}\"] = (0.0, float(y_positions[j]))\n",
    "    pos[rule_node] = (1.2, 0.5)\n",
    "    pos[out_node]  = (2.4, 0.5)\n",
    "\n",
    "    # Node colors\n",
    "    node_colors = []\n",
    "    for n in G.nodes():\n",
    "        kind = G.nodes[n][\"kind\"]\n",
    "        if kind == \"antecedent\":\n",
    "            imp = float(G.nodes[n][\"impact\"])\n",
    "            node_colors.append(IMPACT_CMAP(norm(imp)))  # heatmap\n",
    "        elif kind == \"rule\":\n",
    "            node_colors.append(\"#8ecae6\")  # light blue\n",
    "        else:\n",
    "            node_colors.append(CLASS_COLOR[cls])        # green/orange output\n",
    "\n",
    "    labels = {n: G.nodes[n][\"label\"] for n in G.nodes()}\n",
    "    edge_widths = [0.8 + 2.0 * G.edges[e][\"weight\"] for e in G.edges()]\n",
    "\n",
    "\n",
    "    nx.draw(\n",
    "        G, pos, ax=ax,\n",
    "        with_labels=True,\n",
    "        labels=labels,\n",
    "        node_size=2600,\n",
    "        node_color=node_colors,\n",
    "        font_size=9,\n",
    "        arrows=True,\n",
    "        width=edge_widths\n",
    "    )\n",
    "    ax.set_title(f\"Most positive vs most negative — rule {rule_index+1}\", fontsize=11)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    return cls, v\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main: pick best positive and best negative rules\n",
    "# -----------------------------\n",
    "V = safe_get_V(model)\n",
    "W = safe_get_W(model)\n",
    "\n",
    "pos_idx = int(np.argmax(V))\n",
    "neg_idx = int(np.argmin(V))\n",
    "\n",
    "# Normalize impacts for heatmap (across ALL rules/features for consistent colors)\n",
    "imp_all = W.flatten()\n",
    "norm = Normalize(vmin=float(np.min(imp_all)), vmax=float(np.max(imp_all) + 1e-12))\n",
    "\n",
    "print(\"✓ Selected rules:\")\n",
    "print(\"  Most positive V  -> rule\", pos_idx+1, \"V =\", float(V[pos_idx]))\n",
    "print(\"  Most negative V  -> rule\", neg_idx+1, \"V =\", float(V[neg_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Plot: two panels (one below the other)\n",
    "# -----------------------------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 7), constrained_layout=True)\n",
    "\n",
    "cls_pos, v_pos = draw_one_rule(axes[0], model, pos_idx, V, W, norm)\n",
    "cls_neg, v_neg = draw_one_rule(axes[1], model, neg_idx, V, W, norm)\n",
    "\n",
    "axes[0].set_title(f\"Top POSITIVE rule (V={v_pos:.3f}) → {CLASS_NAME[cls_pos]}\", fontsize=12)\n",
    "axes[1].set_title(f\"Top NEGATIVE rule (V={v_neg:.3f}) → {CLASS_NAME[cls_neg]}\", fontsize=12)\n",
    "\n",
    "# Legend: output colors\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=CLASS_COLOR[+1], label=CLASS_NAME[+1]),\n",
    "    mpatches.Patch(color=CLASS_COLOR[-1], label=CLASS_NAME[-1]),\n",
    "]\n",
    "fig.legend(handles=legend_patches, loc=\"lower center\", ncol=2, frameon=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Two-rule comparison plotted (outputs colored; inputs heatmapped by impact)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31089659",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) Interpretability diagnostics (matrices)\n",
    "\n",
    "You mentioned these are central in your model:\n",
    "\n",
    "- **Consistency matrix** (from consequents `V`)\n",
    "- **Similarity matrix** (from rule parameters: centers/sigmas)\n",
    "- **Distinguishability / overlap** (from MF overlap)\n",
    "- **Completeness** (coverage of samples above an activation threshold)\n",
    "\n",
    "This section computes them and plots compact heatmaps (top-left block)\n",
    "so you can show structure without overwhelming the audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4338081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# 0) Try to import your \"official\" interpretability functions\n",
    "# ============================================================\n",
    "try:\n",
    "    from experiments.calculate import (\n",
    "        calculate_consistency_matrix,\n",
    "        calculate_similarity_matrix,\n",
    "        calculate_distinguishability_matrix,\n",
    "        calculate_ecompleteness_and_get_uncovered_samples,\n",
    "        calculate_overlap,\n",
    "    )\n",
    "    print(\"✓ Imported interpretability functions from experiments.calculate\")\n",
    "except Exception as e:\n",
    "    print(\"Project import failed:\", repr(e))\n",
    "    print(\"Using local reference implementations (same spirit as your definitions).\")\n",
    "\n",
    "    def calculate_consistency_matrix(V):\n",
    "        V = np.asarray(V, dtype=float)\n",
    "        if V.ndim == 1:\n",
    "            V = V.reshape(-1, 1)\n",
    "        n = V.shape[0]\n",
    "        C = np.zeros((n, n), dtype=float)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                diff = np.abs(V[i] - V[j])\n",
    "                C[i, j] = C[j, i] = float(np.exp(-diff).mean())\n",
    "        np.fill_diagonal(C, 1.0)\n",
    "        return C\n",
    "\n",
    "    def _ovl_1d(mu1, s1, mu2, s2, n_grid=400):\n",
    "        s1 = max(float(s1), 1e-9)\n",
    "        s2 = max(float(s2), 1e-9)\n",
    "        lo = min(mu1 - 4 * s1, mu2 - 4 * s2)\n",
    "        hi = max(mu1 + 4 * s1, mu2 + 4 * s2)\n",
    "        xs = np.linspace(lo, hi, n_grid)\n",
    "        p1 = (1.0 / (s1 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xs - mu1) / s1) ** 2)\n",
    "        p2 = (1.0 / (s2 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xs - mu2) / s2) ** 2)\n",
    "        return np.trapz(np.minimum(p1, p2), xs)\n",
    "\n",
    "    def calculate_overlap(mu1, sigma1, mu2, sigma2):\n",
    "        mu1 = np.asarray(mu1, dtype=float).ravel()\n",
    "        mu2 = np.asarray(mu2, dtype=float).ravel()\n",
    "        sigma1 = np.asarray(sigma1, dtype=float).ravel()\n",
    "        sigma2 = np.asarray(sigma2, dtype=float).ravel()\n",
    "        dims = min(mu1.size, mu2.size, sigma1.size, sigma2.size)\n",
    "        if dims == 0:\n",
    "            return 0.0\n",
    "        return float(np.mean([_ovl_1d(mu1[d], sigma1[d], mu2[d], sigma2[d]) for d in range(dims)]))\n",
    "\n",
    "    def calculate_distinguishability_matrix(rules):\n",
    "        # NOTE: this returns AVERAGE OVERLAP in [0,1]\n",
    "        # Higher overlap => LESS distinguishable.\n",
    "        n = len(rules)\n",
    "        O = np.zeros((n, n), dtype=float)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                mu1 = np.array(rules[i][\"centers\"])\n",
    "                s1  = np.array(rules[i][\"sigmas\"])\n",
    "                mu2 = np.array(rules[j][\"centers\"])\n",
    "                s2  = np.array(rules[j][\"sigmas\"])\n",
    "                ovl = calculate_overlap(mu1, s1, mu2, s2)\n",
    "                O[i, j] = O[j, i] = ovl\n",
    "        np.fill_diagonal(O, 1.0)\n",
    "        return O\n",
    "\n",
    "    def calculate_similarity_matrix(rules):\n",
    "        n = len(rules)\n",
    "        S = np.zeros((n, n), dtype=float)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                ci = np.array(rules[i][\"centers\"], dtype=float)\n",
    "                si = np.array(rules[i][\"sigmas\"], dtype=float)\n",
    "                cj = np.array(rules[j][\"centers\"], dtype=float)\n",
    "                sj = np.array(rules[j][\"sigmas\"], dtype=float)\n",
    "                dist = np.sqrt(np.sum((ci - cj) ** 2 + (si - sj) ** 2))\n",
    "                S[i, j] = S[j, i] = float(np.exp(-dist))\n",
    "        np.fill_diagonal(S, 1.0)\n",
    "        return S\n",
    "\n",
    "    def calculate_ecompleteness_and_get_uncovered_samples(all_activations, epsilon):\n",
    "        A_ = np.asarray(all_activations, dtype=float)\n",
    "        is_act = A_ >= float(epsilon)\n",
    "        completeness = float(np.any(is_act, axis=1).mean())\n",
    "        uncovered = np.where(~np.any(is_act, axis=1))[0]\n",
    "        return completeness, uncovered\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Get rules (dict or list) + consequents V\n",
    "# ============================================================\n",
    "rules_obj = model.generate_rules_dictionary()\n",
    "\n",
    "if isinstance(rules_obj, dict):\n",
    "    rules_list = list(rules_obj.values())\n",
    "elif isinstance(rules_obj, list):\n",
    "    rules_list = rules_obj\n",
    "else:\n",
    "    raise TypeError(f\"Unexpected return type from generate_rules_dictionary(): {type(rules_obj)}\")\n",
    "\n",
    "print(\"✓ Rules container type:\", type(rules_obj).__name__)\n",
    "print(\"✓ Number of rules:\", len(rules_list))\n",
    "print(\"✓ One rule keys:\", list(rules_list[0].keys()) if len(rules_list) > 0 else \"N/A\")\n",
    "\n",
    "V = np.asarray(getattr(model, \"OldV\", getattr(model, \"V\")), dtype=float)\n",
    "if V.ndim > 1:\n",
    "    V = V[:, 0]\n",
    "V = V.reshape(-1, 1)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Compute matrices\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "def calculate_consistency_matrix_safe(V):\n",
    "    \"\"\"\n",
    "    Safe version:\n",
    "    returns a float consistency in [0,1] for each rule pair.\n",
    "    Works for V shaped (n_rules,), (n_rules,1) or (n_rules,k).\n",
    "    \"\"\"\n",
    "    V = np.asarray(V, dtype=float)\n",
    "    if V.ndim == 1:\n",
    "        V = V.reshape(-1, 1)\n",
    "\n",
    "    n = V.shape[0]\n",
    "    C = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            diff = np.abs(V[i] - V[j])              # vector if multi-dim\n",
    "            cons = np.exp(-diff).mean()             # <-- FORCE scalar\n",
    "            C[i, j] = C[j, i] = float(cons)\n",
    "\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    return C\n",
    "\n",
    "# Use the safe version in the notebook\n",
    "calculate_consistency_matrix = calculate_consistency_matrix_safe\n",
    "print(\"✓ Patched calculate_consistency_matrix to a scalar-safe version\")\n",
    "\n",
    "C = calculate_consistency_matrix(V)          # higher = more consistent\n",
    "S = calculate_similarity_matrix(rules_list)  # higher = more similar\n",
    "O = calculate_distinguishability_matrix(rules_list)  # higher = more overlap (less distinguishable)\n",
    "\n",
    "print(\"✓ Matrices computed\")\n",
    "print(\"C shape:\", C.shape, \"| S shape:\", S.shape, \"| O shape:\", O.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Plot helper with numbers + blue-red colors\n",
    "# ============================================================\n",
    "def show_matrix_with_values(M, title, max_show=16, vmin=0, vmax=1, fmt=\"{:.2f}\", cmap=\"RdBu_r\"):\n",
    "    \"\"\"\n",
    "    Blue = low values, Red = high values.\n",
    "    Numbers are printed in each cell for clarity.\n",
    "    \"\"\"\n",
    "    n = min(max_show, M.shape[0])\n",
    "    A = M[:n, :n]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    im = ax.imshow(A, cmap=cmap, vmin=vmin, vmax=vmax, interpolation=\"nearest\")\n",
    "\n",
    "    ax.set_title(f\"{title} (top-left {n}×{n})\", fontsize=12)\n",
    "    ax.set_xlabel(\"Rule index\")\n",
    "    ax.set_ylabel(\"Rule index\")\n",
    "\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(range(1, n + 1))\n",
    "    ax.set_yticklabels(range(1, n + 1))\n",
    "\n",
    "    # numbers inside cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            val = float(A[i, j])\n",
    "            # choose text color for readability\n",
    "            norm_val = (val - vmin) / (vmax - vmin + 1e-12)\n",
    "            text_color = \"white\" if norm_val > 0.65 else \"black\"\n",
    "            ax.text(j, i, fmt.format(val), ha=\"center\", va=\"center\", fontsize=8, color=text_color)\n",
    "\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Interpretation helper (prints where things are high/low)\n",
    "# ============================================================\n",
    "def summarize_matrix(M, name, high_means, low_means, top_k=5):\n",
    "    \"\"\"\n",
    "    Prints the most extreme off-diagonal pairs:\n",
    "    - highest values (most similar/consistent/overlapping)\n",
    "    - lowest values (least similar/consistent/overlapping)\n",
    "    \"\"\"\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    n = M.shape[0]\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pairs.append((float(M[i, j]), i, j))\n",
    "\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "\n",
    "    lowest = pairs_sorted[:top_k]\n",
    "    highest = pairs_sorted[-top_k:][::-1]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{name} — How to read it\")\n",
    "    print(f\"  High values (red) mean: {high_means}\")\n",
    "    print(f\"  Low values  (blue) mean: {low_means}\")\n",
    "\n",
    "    print(f\"\\nTop {top_k} HIGHEST off-diagonal pairs:\")\n",
    "    for val, i, j in highest:\n",
    "        print(f\"  Rules ({i+1}, {j+1}) = {val:.3f}\")\n",
    "\n",
    "    print(f\"\\nTop {top_k} LOWEST off-diagonal pairs:\")\n",
    "    for val, i, j in lowest:\n",
    "        print(f\"  Rules ({i+1}, {j+1}) = {val:.3f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Show matrices + explanations (consistent style)\n",
    "# ============================================================\n",
    "MAX_SHOW = 16  # good for seminar projector; increase if you want\n",
    "\n",
    "show_matrix_with_values(C, \"Consistency matrix C (consequents)\", max_show=MAX_SHOW, vmin=0, vmax=1)\n",
    "summarize_matrix(\n",
    "    C,\n",
    "    \"Consistency matrix C\",\n",
    "    high_means=\"rules have very similar consequents (similar outputs) → more consistent\",\n",
    "    low_means=\"rules have very different consequents → less consistent\",\n",
    ")\n",
    "\n",
    "show_matrix_with_values(S, \"Similarity matrix S (centers + sigmas)\", max_show=MAX_SHOW, vmin=0, vmax=1)\n",
    "summarize_matrix(\n",
    "    S,\n",
    "    \"Similarity matrix S\",\n",
    "    high_means=\"rules have very similar antecedent parameters (centers/sigmas) → redundant structure\",\n",
    "    low_means=\"rules are structurally different in antecedents\",\n",
    ")\n",
    "\n",
    "show_matrix_with_values(O, \"Overlap matrix O (Gaussian overlap)\", max_show=MAX_SHOW, vmin=0, vmax=1)\n",
    "summarize_matrix(\n",
    "    O,\n",
    "    \"Overlap matrix O\",\n",
    "    high_means=\"rules have high overlap → LESS distinguishable (they 'cover' similar regions)\",\n",
    "    low_means=\"rules have low overlap → MORE distinguishable (better separation)\",\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6) e-Completeness (coverage of samples by rules)\n",
    "# ============================================================\n",
    "epsilon = 1e-6  # threshold for \"a rule is activated\"\n",
    "completeness, uncovered = calculate_ecompleteness_and_get_uncovered_samples(A, epsilon)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"e-Completeness (coverage)\")\n",
    "print(f\"  epsilon threshold: {epsilon}\")\n",
    "print(\"  Interpretation:\")\n",
    "print(\"   - completeness close to 1.0 means: almost every sample activates at least one rule\")\n",
    "print(\"   - low completeness means: many samples are not covered (no rule strongly activates)\")\n",
    "print(f\"\\n  completeness = {completeness:.3f}\")\n",
    "print(f\"  uncovered samples = {len(uncovered)} / {A.shape[0]}\")\n",
    "\n",
    "# Show a few uncovered indices for discussion\n",
    "if len(uncovered) > 0:\n",
    "    print(\"  first uncovered sample indices:\", uncovered[:10])\n",
    "else:\n",
    "    print(\"  ✓ no uncovered samples at this epsilon\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "print(\"\\n✓ Interpretability diagnostics done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb2d01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) Research extensions (talking points)\n",
    "\n",
    "- **Different fuzzification**: triangular / trapezoidal / bell-shaped MFs  \n",
    "- **Different aggregation**: alternative t-norms / s-norms / parameterized uninorms  \n",
    "- **Different consequent learning**: PSO / gradient-based optimization / regularization  \n",
    "- **Pruning & merging**: use the matrices above to reduce redundancy and improve interpretability  \n",
    "- **Day 2 bridge**: in streams with drift, rules should be able to **adapt over time** (evolving fuzzy systems)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
